{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d5748d-4e0d-4f8d-8029-865f4c85b3b4",
   "metadata": {},
   "source": [
    "# 04 Classification Models using bootstrapping\n",
    "\n",
    "This code aims to train classification models using bootstrapping to recognize the causality sentences from wikipages\n",
    "\n",
    "* **Input**: The semEval dataframes; The wikipages dataframes; the seed pairs in list\n",
    "* **Approaches**: get the sentence embeddings to train on the *Logistic Regression* classifier in iterrations with bootstrapping; apply the *LSTM classifier* with specific sentence embedding techniques in iterrations with bootstrapping\n",
    "* **Output**: Evaluate the end models with different metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ba3b12-8415-428d-91a1-51bbc53b0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import and install necessary packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument  # sentece to vec\n",
    "\n",
    "from sklearn import metrics # for evaluation\n",
    "from sklearn.linear_model import LogisticRegression # classifer\n",
    "\n",
    "import spacy\n",
    "nlp= spacy.load('en_core_web_sm') # tokenized sentence\n",
    "\n",
    "path_here = os.getcwd()\n",
    "\n",
    "# to install if you don't install yet\n",
    "# !{sys.executable} -m pip install tensorflow\n",
    "from tf_model_03 import get_model, get_feature_arrays\n",
    "from utils_03 import get_n_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c2173f-4889-456f-9061-75a005301694",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the initialized parameters\n",
    "\n",
    "df_res_dev = pd.DataFrame(columns = ['tag', 'Nb:allsamp,predpos,realpos,overlap', 'accuracy', 'precision',\n",
    "                                    'recall', 'F1'])\n",
    "### get the sentences of semEval\n",
    "df_train_semEval = pd.read_pickle(path_here+ \"/res/df_train_semEval.pkl\")\n",
    "df_test_semEval = pd.read_pickle(path_here+ \"/res/df_test_semEval.pkl\")\n",
    "\n",
    "df_enwiki_causality_AA = pd.read_csv(path_here + '/res/df_enwiki_causality_AA.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bad169-6de9-49d9-baf0-1cc56f544c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### purpose: get the embedding of the subset sentences \n",
    "### input: df_subset, the trained whole model\n",
    "### output: the embedding feature space\n",
    "def GetEmb(df_subset, model):\n",
    "\n",
    "    wv_doc2vec = []\n",
    "    for inx in df_subset.index.tolist():\n",
    "        wv_doc2vec.append(model.dv[inx])\n",
    "    return np.array(wv_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c970aeb2-e671-4bde-af25-3268f83072f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished Round 0\n",
      "finished Round 1\n",
      "finished Round 2\n",
      "finished Round 3\n",
      "finished Round 4\n"
     ]
    }
   ],
   "source": [
    "### train the Logistic Regression Classifier\n",
    "\n",
    "\n",
    "### the initialized parameters\n",
    "random_times = 3\n",
    "delta_ls = [] # the increased postive examples in different round\n",
    "df_train_semEval_b1 = pd.DataFrame()\n",
    "df_res_dev_boot = pd.DataFrame(columns = ['tag', 'Nb:allsamp,predpos,realpos,overlap', 'accuracy', 'precision',\n",
    "                                    'recall', 'F1'])\n",
    "\n",
    "\n",
    "for rd in range(5):\n",
    "\n",
    "    delta_ls.append(int(len(df_train_semEval_b1)/2))\n",
    "\n",
    "    df_train_semEval = pd.concat([df_train_semEval, df_train_semEval_b1], ignore_index=True)\n",
    "    df_train_semEval.index = range(len(df_train_semEval))\n",
    "    df_test_semEval.index = range(len(df_train_semEval),len(df_train_semEval)+len(df_test_semEval))\n",
    "\n",
    "    Y_train_semEval = df_train_semEval['Label'].tolist()\n",
    "    Y_test_semEval = df_test_semEval['Label'].tolist()\n",
    "    \n",
    "    \n",
    "    for random_s in range(random_times):\n",
    "        ### try to embedding the sentences by doc2ve\n",
    "        doc_list = df_train_semEval['Sent'].tolist()\n",
    "        doc_list.extend(df_test_semEval['Sent'].tolist())\n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc_list)]\n",
    "        model_doc2vec = Doc2Vec(documents, vector_size=50, window=5, min_count=2, workers=4)\n",
    "\n",
    "        ### get the sentence embedding \n",
    "        wv_semEval_train = GetEmb(df_train_semEval, model_doc2vec)\n",
    "        ### get the sentence embedding of test set\n",
    "        wv_semEval_test = GetEmb(df_test_semEval, model_doc2vec)\n",
    "\n",
    "\n",
    "\n",
    "        ### train the LogisticRegression classifer and evaluate\n",
    "        X = wv_semEval_train\n",
    "        y = Y_train_semEval\n",
    "        clf_lr = LogisticRegression(random_state=0).fit(X, y)\n",
    "        ### test this classifer\n",
    "        NameTag = 'LR_Round{}Random{}'.format(rd, random_s)\n",
    "        X_test = wv_semEval_test\n",
    "        preds_test = clf_lr.predict(X_test)\n",
    "        probs_test = clf_lr.predict_proba(X_test)\n",
    "        df_res_dev_boot = df_res_dev_boot.append({'tag': NameTag,\n",
    "                                        # all samples; predicted postive , real postive, overlap postive\n",
    "                                        'Nb:allsamp,predpos,realpos,overlap': [len(preds_test), Counter(preds_test)[1], Counter(Y_test_semEval)[1], \n",
    "                                        len([i for inx, i in enumerate(preds_test) if i == Y_test_semEval[inx]])],\n",
    "                                        # metrics values\n",
    "                                        'accuracy': metrics.accuracy_score(Y_test_semEval, preds_test), \n",
    "                                        'precision': metrics.precision_score(Y_test_semEval, preds_test),\n",
    "                                        'recall': metrics.recall_score(Y_test_semEval, preds_test),\n",
    "                                        'F1': metrics.f1_score(Y_test_semEval, preds_test),\n",
    "                                       }, ignore_index=True)\n",
    "    \n",
    "\n",
    "    #### add the sentences that includes the seed pairs as positives\n",
    "    pos_inx_all = np.where(df_enwiki_causality_AA['SeedTF']==1)[0]\n",
    "    pos_inx_n_all = np.where(df_enwiki_causality_AA['SeedTF']==0)[0]\n",
    "\n",
    "    nb = 100\n",
    "    # put the sentenes inside the dataframes\n",
    "    df_train_semEval_b1 = pd.DataFrame(columns = df_train_semEval.columns)\n",
    "    # positive rows in dataframe\n",
    "    for inx, row in df_enwiki_causality_AA.loc[pos_inx_all[nb*rd: nb*(rd+1)]].iterrows():\n",
    "        df_train_semEval_b1 = df_train_semEval_b1.append({'SentID': None, 'Cause': row['pairs'][0], 'Effect': row['pairs'][1],'Label':1, 'Sent': row['sentence']}, ignore_index=True)\n",
    "    # negative rows in dataframe (the same as positive)\n",
    "    nb = len(df_train_semEval_b1)\n",
    "    for inx, row in df_enwiki_causality_AA.loc[pos_inx_n_all[nb*rd: nb*(rd+1)]].iterrows():\n",
    "        df_train_semEval_b1 = df_train_semEval_b1.append({'SentID': None, 'Cause': row['pairs'][0], 'Effect': row['pairs'][1],'Label':0, 'Sent': row['sentence']}, ignore_index=True)\n",
    "\n",
    "    \n",
    "    print(\"finished Round {}\".format(rd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e63ab5c-7b85-4cdb-a12f-45a31eafb1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>Nb:allsamp,predpos,realpos,overlap</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Round0Random0</td>\n",
       "      <td>[656, 260, 328, 352]</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>0.482993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR_Round0Random1</td>\n",
       "      <td>[656, 260, 328, 350]</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.542308</td>\n",
       "      <td>0.429878</td>\n",
       "      <td>0.479592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR_Round0Random2</td>\n",
       "      <td>[656, 255, 328, 355]</td>\n",
       "      <td>0.541159</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>0.429878</td>\n",
       "      <td>0.483705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR_Round1Random0</td>\n",
       "      <td>[656, 267, 328, 371]</td>\n",
       "      <td>0.565549</td>\n",
       "      <td>0.580524</td>\n",
       "      <td>0.472561</td>\n",
       "      <td>0.521008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR_Round1Random1</td>\n",
       "      <td>[656, 257, 328, 361]</td>\n",
       "      <td>0.550305</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.442073</td>\n",
       "      <td>0.495726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR_Round1Random2</td>\n",
       "      <td>[656, 255, 328, 365]</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>0.572549</td>\n",
       "      <td>0.445122</td>\n",
       "      <td>0.500858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR_Round2Random0</td>\n",
       "      <td>[656, 282, 328, 374]</td>\n",
       "      <td>0.570122</td>\n",
       "      <td>0.581560</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.537705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR_Round2Random1</td>\n",
       "      <td>[656, 268, 328, 372]</td>\n",
       "      <td>0.567073</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.475610</td>\n",
       "      <td>0.523490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LR_Round2Random2</td>\n",
       "      <td>[656, 269, 328, 383]</td>\n",
       "      <td>0.583841</td>\n",
       "      <td>0.602230</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.542714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LR_Round3Random0</td>\n",
       "      <td>[656, 308, 328, 370]</td>\n",
       "      <td>0.564024</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>0.550314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LR_Round3Random1</td>\n",
       "      <td>[656, 276, 328, 366]</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.568841</td>\n",
       "      <td>0.478659</td>\n",
       "      <td>0.519868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LR_Round3Random2</td>\n",
       "      <td>[656, 251, 328, 357]</td>\n",
       "      <td>0.544207</td>\n",
       "      <td>0.557769</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.483592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LR_Round4Random0</td>\n",
       "      <td>[656, 279, 328, 369]</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.573477</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.527183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LR_Round4Random1</td>\n",
       "      <td>[656, 283, 328, 359]</td>\n",
       "      <td>0.547256</td>\n",
       "      <td>0.554770</td>\n",
       "      <td>0.478659</td>\n",
       "      <td>0.513912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LR_Round4Random2</td>\n",
       "      <td>[656, 299, 328, 373]</td>\n",
       "      <td>0.568598</td>\n",
       "      <td>0.575251</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.548644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tag Nb:allsamp,predpos,realpos,overlap  accuracy  precision  \\\n",
       "0   LR_Round0Random0               [656, 260, 328, 352]  0.536585   0.546154   \n",
       "1   LR_Round0Random1               [656, 260, 328, 350]  0.533537   0.542308   \n",
       "2   LR_Round0Random2               [656, 255, 328, 355]  0.541159   0.552941   \n",
       "3   LR_Round1Random0               [656, 267, 328, 371]  0.565549   0.580524   \n",
       "4   LR_Round1Random1               [656, 257, 328, 361]  0.550305   0.564202   \n",
       "5   LR_Round1Random2               [656, 255, 328, 365]  0.556402   0.572549   \n",
       "6   LR_Round2Random0               [656, 282, 328, 374]  0.570122   0.581560   \n",
       "7   LR_Round2Random1               [656, 268, 328, 372]  0.567073   0.582090   \n",
       "8   LR_Round2Random2               [656, 269, 328, 383]  0.583841   0.602230   \n",
       "9   LR_Round3Random0               [656, 308, 328, 370]  0.564024   0.568182   \n",
       "10  LR_Round3Random1               [656, 276, 328, 366]  0.557927   0.568841   \n",
       "11  LR_Round3Random2               [656, 251, 328, 357]  0.544207   0.557769   \n",
       "12  LR_Round4Random0               [656, 279, 328, 369]  0.562500   0.573477   \n",
       "13  LR_Round4Random1               [656, 283, 328, 359]  0.547256   0.554770   \n",
       "14  LR_Round4Random2               [656, 299, 328, 373]  0.568598   0.575251   \n",
       "\n",
       "      recall        F1  \n",
       "0   0.432927  0.482993  \n",
       "1   0.429878  0.479592  \n",
       "2   0.429878  0.483705  \n",
       "3   0.472561  0.521008  \n",
       "4   0.442073  0.495726  \n",
       "5   0.445122  0.500858  \n",
       "6   0.500000  0.537705  \n",
       "7   0.475610  0.523490  \n",
       "8   0.493902  0.542714  \n",
       "9   0.533537  0.550314  \n",
       "10  0.478659  0.519868  \n",
       "11  0.426829  0.483592  \n",
       "12  0.487805  0.527183  \n",
       "13  0.478659  0.513912  \n",
       "14  0.524390  0.548644  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save to disk\n",
    "df_res_dev_boot.to_pickle(path_here+ \"/res/df_res_dev_boot_lr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3318fe4c-a81a-4bf5-9fcb-5adeeabe6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### purpose: to find the index of NPs pairs in sentences\n",
    "### input: The SemEval Dataframe\n",
    "### output: The SemEval Dataframe with 3 more columns <'ele1_word_idx', 'ele2_word_idx', 'tokens'>\n",
    "\n",
    "def addInxcolDf(df_train_filtered):\n",
    "    # be aware it will be the new dataframe\n",
    "    new_list = df_train_filtered.columns.to_list()\n",
    "    t_list = ['ele1_word_idx', 'ele2_word_idx', 'tokens']\n",
    "    new_list.extend(t_list)\n",
    "    df = pd.DataFrame(columns = new_list)\n",
    "    \n",
    "    # find index of all tokens in one NP \n",
    "    def findInx(tokens):\n",
    "        ls = ['Cause', 'Effect']\n",
    "        res = []\n",
    "        for v in ls: \n",
    "            ele_A = re.findall(r\"[\\w']+|[.,!?;-]\", rows[v])\n",
    "            try: \n",
    "                inx_l = tokens.index(ele_A[0])\n",
    "                inx_r = tokens.index(ele_A[-1])\n",
    "                tu_A = (inx_l, inx_r)\n",
    "            except:\n",
    "                tu_A = (0, 0)\n",
    "            res.append(tu_A)\n",
    "        return res[0], res[1]\n",
    "    \n",
    "\n",
    "    for ind, rows in df_train_filtered.iterrows():\n",
    "        doc = nlp(rows['Sent'])\n",
    "        tokens = [token.text.lower() for token in doc]\n",
    "\n",
    "        tu_A, tu_B = findInx(tokens)\n",
    "        # ensure left index is the smallest\n",
    "        if tu_A[0] > tu_B[0]:\n",
    "            ele1_word_idx = tu_B\n",
    "            ele2_word_idx = tu_A            \n",
    "        else:\n",
    "            ele1_word_idx = tu_A\n",
    "            ele2_word_idx = tu_B\n",
    "\n",
    "        ### add the new row\n",
    "        dict_newrow = dict(rows)\n",
    "        dict_newrow.update({ 'ele1_word_idx': ele1_word_idx, \n",
    "                  'ele2_word_idx': ele2_word_idx, 'tokens': tokens})\n",
    "\n",
    "        ### new dataframe\n",
    "        df = df.append(dict_newrow, ignore_index = True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "### purpose: add 3 more columns of SemEval Dataframe to prepare the inputting data for LSTM\n",
    "### input: The SemEval Dataframe with 3 more columns <'ele1_word_idx', 'ele2_word_idx', 'tokens'>\n",
    "### output: The SemEval Dataframe with extra columns <'text_between', 'ele1_left_tokens', 'ele2_right_tokens'>\n",
    "\n",
    "def add3colDf(df_train_filtered):\n",
    "    # be aware it will be the new dataframe\n",
    "    new_list = df_train_filtered.columns.to_list()\n",
    "    t_list = ['text_between', 'ele1_left_tokens', 'ele2_right_tokens']\n",
    "    new_list.extend(t_list)\n",
    "    df = pd.DataFrame(columns = new_list)\n",
    "\n",
    "    for ind, rows in df_train_filtered.iterrows():\n",
    "        text_between = rows['tokens'][rows['ele1_word_idx'][1]+1: rows['ele2_word_idx'][0]]\n",
    "        ele1_left_tokens = rows['tokens'][:rows['ele1_word_idx'][0]]\n",
    "        ele2_right_tokens = rows['tokens'][rows['ele2_word_idx'][1]+1:]\n",
    "\n",
    "        ### add the new row\n",
    "        dict_newrow = dict(rows)\n",
    "        dict_newrow.update({ 'ele1_left_tokens': ele1_left_tokens, \n",
    "                  'text_between': text_between, 'ele2_right_tokens': ele2_right_tokens})\n",
    "\n",
    "        ### new dataframe\n",
    "        df = df.append(dict_newrow, ignore_index = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073fcfb-04f4-439d-a7d3-6dfcf6592af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/zoe/Desktop/codes/GSOC_RelationExtraction_github/tf_model_03.py:69: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/30\n",
      "2806/2806 [==============================] - 10s 3ms/sample - loss: 0.6940\n",
      "Epoch 2/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6935\n",
      "Epoch 3/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6937\n",
      "Epoch 4/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6932\n",
      "Epoch 5/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6930\n",
      "Epoch 6/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6923\n",
      "Epoch 7/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6911\n",
      "Epoch 8/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6874\n",
      "Epoch 9/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6810\n",
      "Epoch 10/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6634\n",
      "Epoch 11/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.5920\n",
      "Epoch 12/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.5188\n",
      "Epoch 13/30\n",
      "2806/2806 [==============================] - 9s 3ms/sample - loss: 0.4686\n",
      "Epoch 14/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.4264\n",
      "Epoch 15/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.3912\n",
      "Epoch 16/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.3542\n",
      "Epoch 17/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.3424\n",
      "Epoch 18/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.2855\n",
      "Epoch 19/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.2722\n",
      "Epoch 20/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.2097\n",
      "Epoch 21/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.1752\n",
      "Epoch 22/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.1357\n",
      "Epoch 23/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.0968\n",
      "Epoch 24/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.0542\n",
      "Epoch 25/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.0410\n",
      "Epoch 26/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.0652\n",
      "Epoch 27/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.0307\n",
      "Epoch 28/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.0178\n",
      "Epoch 29/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.0147\n",
      "Epoch 30/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.0105\n",
      "Epoch 1/30\n",
      "2806/2806 [==============================] - 10s 4ms/sample - loss: 0.6935\n",
      "Epoch 2/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6936\n",
      "Epoch 3/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.6937\n",
      "Epoch 4/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.6935\n",
      "Epoch 5/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6934\n",
      "Epoch 6/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.6934\n",
      "Epoch 7/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6934\n",
      "Epoch 8/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6929\n",
      "Epoch 9/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6919\n",
      "Epoch 10/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6903\n",
      "Epoch 11/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6863\n",
      "Epoch 12/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6789\n",
      "Epoch 13/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6428\n",
      "Epoch 14/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.5511\n",
      "Epoch 15/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.4975\n",
      "Epoch 16/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.4515\n",
      "Epoch 17/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.4123\n",
      "Epoch 18/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.3793\n",
      "Epoch 19/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.3512\n",
      "Epoch 20/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.3227\n",
      "Epoch 21/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.2959\n",
      "Epoch 22/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.2628\n",
      "Epoch 23/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.2340\n",
      "Epoch 24/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.1854\n",
      "Epoch 25/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.1347\n",
      "Epoch 26/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.1007\n",
      "Epoch 27/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.0676\n",
      "Epoch 28/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.0576\n",
      "Epoch 29/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.0375\n",
      "Epoch 30/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.0244\n",
      "Epoch 1/30\n",
      "2806/2806 [==============================] - 11s 4ms/sample - loss: 0.6941\n",
      "Epoch 2/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6935\n",
      "Epoch 3/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.6929\n",
      "Epoch 4/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6936\n",
      "Epoch 5/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.6934\n",
      "Epoch 6/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.6926\n",
      "Epoch 7/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6926\n",
      "Epoch 8/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.6913\n",
      "Epoch 9/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.6907\n",
      "Epoch 10/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6865\n",
      "Epoch 11/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.6766\n",
      "Epoch 12/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.6355\n",
      "Epoch 13/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.5486\n",
      "Epoch 14/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.4799\n",
      "Epoch 15/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.4470\n",
      "Epoch 16/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.3971\n",
      "Epoch 17/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.3660\n",
      "Epoch 18/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.3538\n",
      "Epoch 19/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.3224\n",
      "Epoch 20/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.2857\n",
      "Epoch 21/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.2461\n",
      "Epoch 22/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.2160\n",
      "Epoch 23/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.1913\n",
      "Epoch 24/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.1326\n",
      "Epoch 25/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.0874\n",
      "Epoch 26/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.0746\n",
      "Epoch 27/30\n",
      "2806/2806 [==============================] - 6s 2ms/sample - loss: 0.0434\n",
      "Epoch 28/30\n",
      "2806/2806 [==============================] - 7s 3ms/sample - loss: 0.0664\n",
      "Epoch 29/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.0396\n",
      "Epoch 30/30\n",
      "2806/2806 [==============================] - 7s 2ms/sample - loss: 0.0222\n",
      "finished Round 0\n",
      "Epoch 1/30\n",
      "3006/3006 [==============================] - 12s 4ms/sample - loss: 0.6935\n",
      "Epoch 2/30\n",
      "3006/3006 [==============================] - 8s 3ms/sample - loss: 0.6927\n",
      "Epoch 3/30\n",
      "3006/3006 [==============================] - 8s 3ms/sample - loss: 0.6904\n",
      "Epoch 4/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6888\n",
      "Epoch 5/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6806\n",
      "Epoch 6/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6512\n",
      "Epoch 7/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.5826\n",
      "Epoch 8/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.5224\n",
      "Epoch 9/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.4651\n",
      "Epoch 10/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.4390\n",
      "Epoch 11/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.4043\n",
      "Epoch 12/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.3615\n",
      "Epoch 13/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.3336\n",
      "Epoch 14/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.3015\n",
      "Epoch 15/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.2190\n",
      "Epoch 16/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.1710\n",
      "Epoch 17/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.1277\n",
      "Epoch 18/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0716\n",
      "Epoch 19/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0486\n",
      "Epoch 20/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.0817\n",
      "Epoch 21/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0494\n",
      "Epoch 22/30\n",
      "3006/3006 [==============================] - 9s 3ms/sample - loss: 0.0220\n",
      "Epoch 23/30\n",
      "3006/3006 [==============================] - 8s 3ms/sample - loss: 0.0162\n",
      "Epoch 24/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0135\n",
      "Epoch 25/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.0110\n",
      "Epoch 26/30\n",
      "3006/3006 [==============================] - 8s 3ms/sample - loss: 0.0099\n",
      "Epoch 27/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.0081\n",
      "Epoch 28/30\n",
      "3006/3006 [==============================] - 8s 3ms/sample - loss: 0.0673\n",
      "Epoch 29/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.0121\n",
      "Epoch 30/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.0075\n",
      "Epoch 1/30\n",
      "3006/3006 [==============================] - 16s 5ms/sample - loss: 0.6936\n",
      "Epoch 2/30\n",
      "3006/3006 [==============================] - 11s 3ms/sample - loss: 0.6934\n",
      "Epoch 3/30\n",
      "3006/3006 [==============================] - 8s 3ms/sample - loss: 0.6933\n",
      "Epoch 4/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6929\n",
      "Epoch 5/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6924\n",
      "Epoch 6/30\n",
      "3006/3006 [==============================] - 8s 3ms/sample - loss: 0.6904\n",
      "Epoch 7/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6891\n",
      "Epoch 8/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6843\n",
      "Epoch 9/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6689\n",
      "Epoch 10/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6164\n",
      "Epoch 11/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.5431\n",
      "Epoch 12/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.4924\n",
      "Epoch 13/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.4516\n",
      "Epoch 14/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.4206\n",
      "Epoch 15/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.3970\n",
      "Epoch 16/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.3686\n",
      "Epoch 17/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.3390\n",
      "Epoch 18/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.2996\n",
      "Epoch 19/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.2413\n",
      "Epoch 20/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.1816\n",
      "Epoch 21/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.1324\n",
      "Epoch 22/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0966\n",
      "Epoch 23/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0582\n",
      "Epoch 24/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0692\n",
      "Epoch 25/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0362\n",
      "Epoch 26/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0679\n",
      "Epoch 27/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0295\n",
      "Epoch 28/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0162\n",
      "Epoch 29/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0132\n",
      "Epoch 30/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.0114\n",
      "Epoch 1/30\n",
      "3006/3006 [==============================] - 12s 4ms/sample - loss: 0.6941\n",
      "Epoch 2/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6936\n",
      "Epoch 3/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6934\n",
      "Epoch 4/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6933\n",
      "Epoch 5/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6935\n",
      "Epoch 6/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6935\n",
      "Epoch 7/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6933\n",
      "Epoch 8/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6934\n",
      "Epoch 9/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6933\n",
      "Epoch 10/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6935\n",
      "Epoch 11/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6933\n",
      "Epoch 12/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6933\n",
      "Epoch 13/30\n",
      "3006/3006 [==============================] - 7s 2ms/sample - loss: 0.6931\n",
      "Epoch 14/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6930\n",
      "Epoch 15/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6929\n",
      "Epoch 16/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6919\n",
      "Epoch 17/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6909\n",
      "Epoch 18/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6868\n",
      "Epoch 19/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6806\n",
      "Epoch 20/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.6474\n",
      "Epoch 21/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.5644\n",
      "Epoch 22/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.4996\n",
      "Epoch 23/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.4599\n",
      "Epoch 24/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.4305\n",
      "Epoch 25/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.3862\n",
      "Epoch 26/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.3421\n",
      "Epoch 27/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.2956\n",
      "Epoch 28/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.2376\n",
      "Epoch 29/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.1894\n",
      "Epoch 30/30\n",
      "3006/3006 [==============================] - 6s 2ms/sample - loss: 0.1318\n",
      "finished Round 1\n",
      "Epoch 1/30\n",
      "3206/3206 [==============================] - 13s 4ms/sample - loss: 0.6931\n",
      "Epoch 2/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6940\n",
      "Epoch 3/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.6935\n",
      "Epoch 4/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6935\n",
      "Epoch 5/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.6935\n",
      "Epoch 6/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6934\n",
      "Epoch 7/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.6933\n",
      "Epoch 8/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6931\n",
      "Epoch 9/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6932\n",
      "Epoch 10/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6935\n",
      "Epoch 11/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6933\n",
      "Epoch 12/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6929\n",
      "Epoch 13/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.6926\n",
      "Epoch 14/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6921\n",
      "Epoch 15/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6917\n",
      "Epoch 16/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6889\n",
      "Epoch 17/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6817\n",
      "Epoch 18/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6635\n",
      "Epoch 19/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.5880\n",
      "Epoch 20/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.5257\n",
      "Epoch 21/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.4842\n",
      "Epoch 22/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.4437\n",
      "Epoch 23/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.4274\n",
      "Epoch 24/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.3815\n",
      "Epoch 25/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.3215\n",
      "Epoch 26/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.2738\n",
      "Epoch 27/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.1890\n",
      "Epoch 28/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.1260\n",
      "Epoch 29/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.0818\n",
      "Epoch 30/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.0509\n",
      "Epoch 1/30\n",
      "3206/3206 [==============================] - 14s 4ms/sample - loss: 0.6936\n",
      "Epoch 2/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6934\n",
      "Epoch 3/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6933\n",
      "Epoch 4/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6930\n",
      "Epoch 5/30\n",
      "3206/3206 [==============================] - 6s 2ms/sample - loss: 0.6928\n",
      "Epoch 6/30\n",
      "3206/3206 [==============================] - 8s 3ms/sample - loss: 0.6924\n",
      "Epoch 7/30\n",
      "3206/3206 [==============================] - 8s 3ms/sample - loss: 0.6913\n",
      "Epoch 8/30\n",
      "3206/3206 [==============================] - 8s 3ms/sample - loss: 0.6912\n",
      "Epoch 9/30\n",
      "3206/3206 [==============================] - 10s 3ms/sample - loss: 0.6860\n",
      "Epoch 10/30\n",
      "3206/3206 [==============================] - 10s 3ms/sample - loss: 0.6762\n",
      "Epoch 11/30\n",
      "3206/3206 [==============================] - 8s 3ms/sample - loss: 0.6328\n",
      "Epoch 12/30\n",
      "3206/3206 [==============================] - 9s 3ms/sample - loss: 0.5568\n",
      "Epoch 13/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.5095\n",
      "Epoch 14/30\n",
      "3206/3206 [==============================] - 8s 2ms/sample - loss: 0.4597\n",
      "Epoch 15/30\n",
      "3206/3206 [==============================] - 8s 2ms/sample - loss: 0.4217\n",
      "Epoch 16/30\n",
      "3206/3206 [==============================] - 8s 2ms/sample - loss: 0.3845\n",
      "Epoch 17/30\n",
      "3206/3206 [==============================] - 9s 3ms/sample - loss: 0.3190\n",
      "Epoch 18/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.2436\n",
      "Epoch 19/30\n",
      "3206/3206 [==============================] - 7s 2ms/sample - loss: 0.1759\n",
      "Epoch 20/30\n",
      "3206/3206 [==============================] - 8s 2ms/sample - loss: 0.1242\n",
      "Epoch 21/30\n",
      " 704/3206 [=====>........................] - ETA: 5s - loss: 0.0725"
     ]
    }
   ],
   "source": [
    "### train the LSTM Classifier\n",
    "\n",
    "### the initialized parameters\n",
    "random_times = 3\n",
    "delta_ls = [] # the increased postive examples in different round\n",
    "df_train_semEval_b1 = pd.DataFrame()\n",
    "df_res_dev_boot = pd.DataFrame(columns = ['tag', 'Nb:allsamp,predpos,realpos,overlap', 'accuracy', 'precision',\n",
    "                                    'recall', 'F1'])\n",
    "\n",
    "\n",
    "for rd in range(5):\n",
    "\n",
    "    \n",
    "    delta_ls.append(int(len(df_train_semEval_b1)/2))\n",
    "\n",
    "    # the inputting dataframes\n",
    "    df_train_semEval = pd.concat([df_train_semEval, df_train_semEval_b1], ignore_index=True)\n",
    "    df_train_semEval.index = range(len(df_train_semEval))\n",
    "    df_test_semEval.index = range(len(df_train_semEval),len(df_train_semEval)+len(df_test_semEval))\n",
    "    Y_train_semEval = df_train_semEval['Label'].tolist()\n",
    "    Y_test_semEval = df_test_semEval['Label'].tolist()\n",
    "    \n",
    "    \n",
    "    # prepare the dataframes for LSTM\n",
    "    df_train_semEval2 = addInxcolDf(df_train_semEval)\n",
    "    df_train_semEval3 = add3colDf(df_train_semEval2)\n",
    "    df_test_semEval2 = addInxcolDf(df_test_semEval)\n",
    "    df_test_semEval3 = add3colDf(df_test_semEval2)\n",
    "    Y_binary = np.array(list(zip([1 if i ==0 else 0 for i in Y_train_semEval], Y_train_semEval)))\n",
    "\n",
    "    \n",
    "    for random_s in range(random_times):\n",
    "        NameTag = 'LSTM_Round{}Random{}'.format(rd, random_s)\n",
    "\n",
    "        ### train the LogisticRegression classifer and evaluate\n",
    "        X_train = get_feature_arrays(df_train_semEval3)\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        model.fit(X_train, Y_binary, batch_size=batch_size, epochs=get_n_epochs())\n",
    "\n",
    "        ### evaluate this classifer\n",
    "        X_test = get_feature_arrays(df_test_semEval3)\n",
    "        probs_test = model.predict(X_test)\n",
    "        preds_test = np.array([1 if r[1] > r[0] else 0 for r in probs_test])\n",
    "\n",
    "        df_res_dev_boot = df_res_dev_boot.append({'tag': NameTag,\n",
    "                                        # TODO: add one column : all samples; predicted postive , real postive, overlap postive\n",
    "                                        'Nb:allsamp,predpos,realpos,overlap': [len(preds_test), Counter(preds_test)[1], Counter(Y_test_semEval)[1], \n",
    "                                        len([i for inx, i in enumerate(preds_test) if i == Y_test_semEval[inx]])],\n",
    "                                        # metrics values\n",
    "                                        'accuracy': metrics.accuracy_score(Y_test_semEval, preds_test), \n",
    "                                        'precision': metrics.precision_score(Y_test_semEval, preds_test),\n",
    "                                        'recall': metrics.recall_score(Y_test_semEval, preds_test),\n",
    "                                        'F1': metrics.f1_score(Y_test_semEval, preds_test),\n",
    "                                       }, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    #### add the sentences that includes the seed pairs as positives\n",
    "    pos_inx_all = np.where(df_enwiki_causality_AA['SeedTF']==1)[0]\n",
    "    pos_inx_n_all = np.where(df_enwiki_causality_AA['SeedTF']==0)[0]\n",
    "\n",
    "    nb = 100\n",
    "    # put the sentenes inside the dataframes\n",
    "    df_train_semEval_b1 = pd.DataFrame(columns = df_train_semEval.columns)\n",
    "    # positive rows in dataframe\n",
    "    for inx, row in df_enwiki_causality_AA.loc[pos_inx_all[nb*rd: nb*(rd+1)]].iterrows():\n",
    "        df_train_semEval_b1 = df_train_semEval_b1.append({'SentID': None, 'Cause': row['pairs'][0], 'Effect': row['pairs'][1],'Label':1, 'Sent': row['sentence']}, ignore_index=True)\n",
    "    # negative rows in dataframe (the same as positive)\n",
    "    nb = len(df_train_semEval_b1)\n",
    "    for inx, row in df_enwiki_causality_AA.loc[pos_inx_n_all[nb*rd: nb*(rd+1)]].iterrows():\n",
    "        df_train_semEval_b1 = df_train_semEval_b1.append({'SentID': None, 'Cause': row['pairs'][0], 'Effect': row['pairs'][1],'Label':0, 'Sent': row['sentence']}, ignore_index=True)\n",
    "\n",
    "    \n",
    "    print(\"finished Round {}\".format(rd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f293ed5-1d75-4df3-a1e2-1331beedb83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
