{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d5748d-4e0d-4f8d-8029-865f4c85b3b4",
   "metadata": {},
   "source": [
    "# 03 Classification Models\n",
    "\n",
    "This code aims to train classification models to recognize the causality sentences from wikipages\n",
    "\n",
    "* **Input**: The semEval dataframes\n",
    "* **Approaches**: get the sentence embeddings to train on the *Logistic Regression* classifier; apply the *LSTM classifier* with specific sentence embedding techniques\n",
    "* **Output**: Evaluate the models with different metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68ba3b12-8415-428d-91a1-51bbc53b0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import and install necessary packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument  # sentece to vec\n",
    "\n",
    "from sklearn import metrics # for evaluation\n",
    "from sklearn.linear_model import LogisticRegression # classifer\n",
    "\n",
    "import spacy\n",
    "nlp= spacy.load('en_core_web_sm') # tokenized sentence\n",
    "\n",
    "path_here = os.getcwd()\n",
    "\n",
    "# to install if you don't install yet\n",
    "# !{sys.executable} -m pip install tensorflow\n",
    "from tf_model_03 import get_model, get_feature_arrays\n",
    "from utils_03 import get_n_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52c2173f-4889-456f-9061-75a005301694",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the initialized parameters\n",
    "\n",
    "df_res_dev = pd.DataFrame(columns = ['tag', 'Nb:allsamp,predpos,realpos,overlap', 'accuracy', 'precision',\n",
    "                                    'recall', 'F1'])\n",
    "### get the sentences of semEval\n",
    "df_train_semEval = pd.read_pickle(path_here+ \"/res/df_train_semEval.pkl\")\n",
    "df_test_semEval = pd.read_pickle(path_here+ \"/res/df_test_semEval.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3bad169-6de9-49d9-baf0-1cc56f544c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### purpose: get the embedding of the subset sentences \n",
    "### input: df_subset, the trained whole model\n",
    "### output: the embedding feature space\n",
    "def GetEmb(df_subset, model):\n",
    "\n",
    "    wv_doc2vec = []\n",
    "    for inx in df_subset.index.tolist():\n",
    "        wv_doc2vec.append(model.dv[inx])\n",
    "    return np.array(wv_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c970aeb2-e671-4bde-af25-3268f83072f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished Round 0\n",
      "finished Round 1\n",
      "finished Round 2\n",
      "finished Round 3\n",
      "finished Round 4\n"
     ]
    }
   ],
   "source": [
    "### train the Logistic Regression Classifier\n",
    "\n",
    "\n",
    "# the inputting dataframes\n",
    "df_train_semEval.index = range(len(df_train_semEval))\n",
    "df_test_semEval.index = range(len(df_train_semEval),len(df_train_semEval)+len(df_test_semEval))\n",
    "Y_train_semEval = df_train_semEval['Label'].tolist()\n",
    "Y_test_semEval = df_test_semEval['Label'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "for rd in range(5):\n",
    "\n",
    "    NameTag = 'LR_Round{}'.format(rd)\n",
    "\n",
    "    ### try to embedding the sentences by doc2ve\n",
    "    doc_list = df_train_semEval['Sent'].tolist()\n",
    "    doc_list.extend(df_test_semEval['Sent'].tolist())\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc_list)]\n",
    "    model_doc2vec = Doc2Vec(documents, vector_size=50, window=5, min_count=2, workers=4)\n",
    "\n",
    "    ### get the sentence embedding \n",
    "    wv_semEval_train = GetEmb(df_train_semEval, model_doc2vec)\n",
    "    ### get the sentence embedding of test set\n",
    "    wv_semEval_test = GetEmb(df_test_semEval, model_doc2vec)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### train the LogisticRegression classifer and evaluate\n",
    "    X = wv_semEval_train\n",
    "    y = Y_train_semEval\n",
    "    clf_lr = LogisticRegression(random_state=0).fit(X, y)\n",
    "    ### test this classifer\n",
    "    X_test = wv_semEval_test\n",
    "    preds_test = clf_lr.predict(X_test)\n",
    "    probs_test = clf_lr.predict_proba(X_test)\n",
    "    df_res_dev = df_res_dev.append({'tag': NameTag,\n",
    "                                    # all samples; predicted postive , real postive, overlap postive\n",
    "                                    'Nb:allsamp,predpos,realpos,overlap': [len(preds_test), Counter(preds_test)[1], Counter(Y_test_semEval)[1], \n",
    "                                    len([i for inx, i in enumerate(preds_test) if i == Y_test_semEval[inx]])],\n",
    "                                    # metrics values\n",
    "                                    'accuracy': metrics.accuracy_score(Y_test_semEval, preds_test), \n",
    "                                    'precision': metrics.precision_score(Y_test_semEval, preds_test),\n",
    "                                    'recall': metrics.recall_score(Y_test_semEval, preds_test),\n",
    "                                    'F1': metrics.f1_score(Y_test_semEval, preds_test),\n",
    "                                   }, ignore_index=True)\n",
    "     \n",
    "    print(\"finished Round {}\".format(rd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e63ab5c-7b85-4cdb-a12f-45a31eafb1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>Nb:allsamp,predpos,realpos,overlap</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Round0</td>\n",
       "      <td>[656, 261, 328, 359]</td>\n",
       "      <td>0.547256</td>\n",
       "      <td>0.559387</td>\n",
       "      <td>0.445122</td>\n",
       "      <td>0.495756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR_Round1</td>\n",
       "      <td>[656, 244, 328, 358]</td>\n",
       "      <td>0.545732</td>\n",
       "      <td>0.561475</td>\n",
       "      <td>0.417683</td>\n",
       "      <td>0.479021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR_Round2</td>\n",
       "      <td>[656, 251, 328, 345]</td>\n",
       "      <td>0.525915</td>\n",
       "      <td>0.533865</td>\n",
       "      <td>0.408537</td>\n",
       "      <td>0.462867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR_Round3</td>\n",
       "      <td>[656, 252, 328, 354]</td>\n",
       "      <td>0.539634</td>\n",
       "      <td>0.551587</td>\n",
       "      <td>0.423780</td>\n",
       "      <td>0.479310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR_Round4</td>\n",
       "      <td>[656, 239, 328, 353]</td>\n",
       "      <td>0.538110</td>\n",
       "      <td>0.552301</td>\n",
       "      <td>0.402439</td>\n",
       "      <td>0.465608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag Nb:allsamp,predpos,realpos,overlap  accuracy  precision  \\\n",
       "0  LR_Round0               [656, 261, 328, 359]  0.547256   0.559387   \n",
       "1  LR_Round1               [656, 244, 328, 358]  0.545732   0.561475   \n",
       "2  LR_Round2               [656, 251, 328, 345]  0.525915   0.533865   \n",
       "3  LR_Round3               [656, 252, 328, 354]  0.539634   0.551587   \n",
       "4  LR_Round4               [656, 239, 328, 353]  0.538110   0.552301   \n",
       "\n",
       "     recall        F1  \n",
       "0  0.445122  0.495756  \n",
       "1  0.417683  0.479021  \n",
       "2  0.408537  0.462867  \n",
       "3  0.423780  0.479310  \n",
       "4  0.402439  0.465608  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3318fe4c-a81a-4bf5-9fcb-5adeeabe6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### purpose: to find the index of NPs pairs in sentences\n",
    "### input: The SemEval Dataframe\n",
    "### output: The SemEval Dataframe with 3 more columns <'ele1_word_idx', 'ele2_word_idx', 'tokens'>\n",
    "\n",
    "def addInxcolDf(df_train_filtered):\n",
    "    # be aware it will be the new dataframe\n",
    "    new_list = df_train_filtered.columns.to_list()\n",
    "    t_list = ['ele1_word_idx', 'ele2_word_idx', 'tokens']\n",
    "    new_list.extend(t_list)\n",
    "    df = pd.DataFrame(columns = new_list)\n",
    "    \n",
    "    # find index of all tokens in one NP \n",
    "    def findInx(tokens):\n",
    "        ls = ['Cause', 'Effect']\n",
    "        res = []\n",
    "        for v in ls: \n",
    "            ele_A = re.findall(r\"[\\w']+|[.,!?;-]\", rows[v])\n",
    "            try: \n",
    "                inx_l = tokens.index(ele_A[0])\n",
    "                inx_r = tokens.index(ele_A[-1])\n",
    "                tu_A = (inx_l, inx_r)\n",
    "            except:\n",
    "                tu_A = (0, 0)\n",
    "            res.append(tu_A)\n",
    "        return res[0], res[1]\n",
    "    \n",
    "\n",
    "    for ind, rows in df_train_filtered.iterrows():\n",
    "        doc = nlp(rows['Sent'])\n",
    "        tokens = [token.text.lower() for token in doc]\n",
    "\n",
    "        tu_A, tu_B = findInx(tokens)\n",
    "        # ensure left index is the smallest\n",
    "        if tu_A[0] > tu_B[0]:\n",
    "            ele1_word_idx = tu_B\n",
    "            ele2_word_idx = tu_A            \n",
    "        else:\n",
    "            ele1_word_idx = tu_A\n",
    "            ele2_word_idx = tu_B\n",
    "\n",
    "        ### add the new row\n",
    "        dict_newrow = dict(rows)\n",
    "        dict_newrow.update({ 'ele1_word_idx': ele1_word_idx, \n",
    "                  'ele2_word_idx': ele2_word_idx, 'tokens': tokens})\n",
    "\n",
    "        ### new dataframe\n",
    "        df = df.append(dict_newrow, ignore_index = True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "### purpose: add 3 more columns of SemEval Dataframe to prepare the inputting data for LSTM\n",
    "### input: The SemEval Dataframe with 3 more columns <'ele1_word_idx', 'ele2_word_idx', 'tokens'>\n",
    "### output: The SemEval Dataframe with extra columns <'text_between', 'ele1_left_tokens', 'ele2_right_tokens'>\n",
    "\n",
    "def add3colDf(df_train_filtered):\n",
    "    # be aware it will be the new dataframe\n",
    "    new_list = df_train_filtered.columns.to_list()\n",
    "    t_list = ['text_between', 'ele1_left_tokens', 'ele2_right_tokens']\n",
    "    new_list.extend(t_list)\n",
    "    df = pd.DataFrame(columns = new_list)\n",
    "\n",
    "    for ind, rows in df_train_filtered.iterrows():\n",
    "        text_between = rows['tokens'][rows['ele1_word_idx'][1]+1: rows['ele2_word_idx'][0]]\n",
    "        ele1_left_tokens = rows['tokens'][:rows['ele1_word_idx'][0]]\n",
    "        ele2_right_tokens = rows['tokens'][rows['ele2_word_idx'][1]+1:]\n",
    "\n",
    "        ### add the new row\n",
    "        dict_newrow = dict(rows)\n",
    "        dict_newrow.update({ 'ele1_left_tokens': ele1_left_tokens, \n",
    "                  'text_between': text_between, 'ele2_right_tokens': ele2_right_tokens})\n",
    "\n",
    "        ### new dataframe\n",
    "        df = df.append(dict_newrow, ignore_index = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d073fcfb-04f4-439d-a7d3-6dfcf6592af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2006/2006 [==============================] - 8s 4ms/sample - loss: 0.6941\n",
      "Epoch 2/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6930\n",
      "Epoch 3/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6934\n",
      "Epoch 4/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6917\n",
      "Epoch 5/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6903\n",
      "Epoch 6/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6864\n",
      "Epoch 7/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6752\n",
      "Epoch 8/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6322\n",
      "Epoch 9/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.5207\n",
      "Epoch 10/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.4326\n",
      "Epoch 11/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.3486\n",
      "Epoch 12/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.3065\n",
      "Epoch 13/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2812\n",
      "Epoch 14/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2257\n",
      "Epoch 15/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1900\n",
      "Epoch 16/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1571\n",
      "Epoch 17/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1482\n",
      "Epoch 18/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1253\n",
      "Epoch 19/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1145\n",
      "Epoch 20/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1293\n",
      "Epoch 21/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1058\n",
      "Epoch 22/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1055\n",
      "Epoch 23/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0989\n",
      "Epoch 24/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0957\n",
      "Epoch 25/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0856\n",
      "Epoch 26/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0876\n",
      "Epoch 27/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0844\n",
      "Epoch 28/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0775\n",
      "Epoch 29/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0758\n",
      "Epoch 30/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0796\n",
      "Epoch 1/30\n",
      "2006/2006 [==============================] - 9s 4ms/sample - loss: 0.6950\n",
      "Epoch 2/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6938\n",
      "Epoch 3/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6937\n",
      "Epoch 4/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6931\n",
      "Epoch 5/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6930\n",
      "Epoch 6/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6930\n",
      "Epoch 7/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6907\n",
      "Epoch 8/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6867\n",
      "Epoch 9/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6748\n",
      "Epoch 10/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6236\n",
      "Epoch 11/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.5150\n",
      "Epoch 12/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.4213\n",
      "Epoch 13/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.3465\n",
      "Epoch 14/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.3125\n",
      "Epoch 15/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2525\n",
      "Epoch 16/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2292\n",
      "Epoch 17/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1836\n",
      "Epoch 18/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1602\n",
      "Epoch 19/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1386\n",
      "Epoch 20/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1250\n",
      "Epoch 21/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1250\n",
      "Epoch 22/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1136\n",
      "Epoch 23/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1037\n",
      "Epoch 24/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1180\n",
      "Epoch 25/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1119\n",
      "Epoch 26/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0920\n",
      "Epoch 27/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0902\n",
      "Epoch 28/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0884\n",
      "Epoch 29/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0840\n",
      "Epoch 30/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0857\n",
      "Epoch 1/30\n",
      "2006/2006 [==============================] - 9s 5ms/sample - loss: 0.6935\n",
      "Epoch 2/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6919\n",
      "Epoch 3/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.6911\n",
      "Epoch 4/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6858\n",
      "Epoch 5/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6673\n",
      "Epoch 6/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.5899\n",
      "Epoch 7/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.4571\n",
      "Epoch 8/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.3965\n",
      "Epoch 9/30\n",
      "2006/2006 [==============================] - 6s 3ms/sample - loss: 0.3438\n",
      "Epoch 10/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.2930\n",
      "Epoch 11/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2445\n",
      "Epoch 12/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2096\n",
      "Epoch 13/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1921\n",
      "Epoch 14/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1580\n",
      "Epoch 15/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1339\n",
      "Epoch 16/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1329\n",
      "Epoch 17/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1174\n",
      "Epoch 18/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1072\n",
      "Epoch 19/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1024\n",
      "Epoch 20/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0999\n",
      "Epoch 21/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1032\n",
      "Epoch 22/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0897\n",
      "Epoch 23/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.0961\n",
      "Epoch 24/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.0834\n",
      "Epoch 25/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0793\n",
      "Epoch 26/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.0784\n",
      "Epoch 27/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0798\n",
      "Epoch 28/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0897\n",
      "Epoch 29/30\n",
      "2006/2006 [==============================] - 6s 3ms/sample - loss: 0.0738\n",
      "Epoch 30/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0687\n",
      "Epoch 1/30\n",
      "2006/2006 [==============================] - 11s 5ms/sample - loss: 0.6934\n",
      "Epoch 2/30\n",
      "2006/2006 [==============================] - 6s 3ms/sample - loss: 0.6932\n",
      "Epoch 3/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6942\n",
      "Epoch 4/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6930\n",
      "Epoch 5/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6924\n",
      "Epoch 6/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6907\n",
      "Epoch 7/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6868\n",
      "Epoch 8/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6717\n",
      "Epoch 9/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.6204\n",
      "Epoch 10/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.4945\n",
      "Epoch 11/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.4269\n",
      "Epoch 12/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.3388\n",
      "Epoch 13/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.3182\n",
      "Epoch 14/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.2480\n",
      "Epoch 15/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2388\n",
      "Epoch 16/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.1729\n",
      "Epoch 17/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1646\n",
      "Epoch 18/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1566\n",
      "Epoch 19/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1260\n",
      "Epoch 20/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1201\n",
      "Epoch 21/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.1086\n",
      "Epoch 22/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1017\n",
      "Epoch 23/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1033\n",
      "Epoch 24/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.1003\n",
      "Epoch 25/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0948\n",
      "Epoch 26/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0969\n",
      "Epoch 27/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0821\n",
      "Epoch 28/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0831\n",
      "Epoch 29/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0861\n",
      "Epoch 30/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0821\n",
      "Epoch 1/30\n",
      "2006/2006 [==============================] - 11s 5ms/sample - loss: 0.6933\n",
      "Epoch 2/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6935\n",
      "Epoch 3/30\n",
      "2006/2006 [==============================] - 6s 3ms/sample - loss: 0.6919\n",
      "Epoch 4/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6914\n",
      "Epoch 5/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6877\n",
      "Epoch 6/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.6800\n",
      "Epoch 7/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.6614\n",
      "Epoch 8/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.5943\n",
      "Epoch 9/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.4634\n",
      "Epoch 10/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.3942\n",
      "Epoch 11/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.3390\n",
      "Epoch 12/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.2982\n",
      "Epoch 13/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.2352\n",
      "Epoch 14/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.2025\n",
      "Epoch 15/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.1691\n",
      "Epoch 16/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.1604\n",
      "Epoch 17/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.1287\n",
      "Epoch 18/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.1305\n",
      "Epoch 19/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.1124\n",
      "Epoch 20/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.1070\n",
      "Epoch 21/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.1043\n",
      "Epoch 22/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.0887\n",
      "Epoch 23/30\n",
      "2006/2006 [==============================] - 5s 3ms/sample - loss: 0.0875\n",
      "Epoch 24/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0848\n",
      "Epoch 25/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0823\n",
      "Epoch 26/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0799\n",
      "Epoch 27/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0848\n",
      "Epoch 28/30\n",
      "2006/2006 [==============================] - 4s 2ms/sample - loss: 0.0719\n",
      "Epoch 29/30\n",
      "2006/2006 [==============================] - 6s 3ms/sample - loss: 0.0750\n",
      "Epoch 30/30\n",
      "2006/2006 [==============================] - 5s 2ms/sample - loss: 0.0722\n"
     ]
    }
   ],
   "source": [
    "### train the LSTM Classifier\n",
    "\n",
    "\n",
    "# the inputting dataframes\n",
    "df_train_semEval.index = range(len(df_train_semEval))\n",
    "df_test_semEval.index = range(len(df_train_semEval),len(df_train_semEval)+len(df_test_semEval))\n",
    "Y_train_semEval = df_train_semEval['Label'].tolist()\n",
    "Y_test_semEval = df_test_semEval['Label'].tolist()\n",
    "\n",
    "\n",
    "# prepare the dataframes for LSTM\n",
    "df_train_semEval2 = addInxcolDf(df_train_semEval)\n",
    "df_train_semEval3 = add3colDf(df_train_semEval2)\n",
    "df_test_semEval2 = addInxcolDf(df_test_semEval)\n",
    "df_test_semEval3 = add3colDf(df_test_semEval2)\n",
    "\n",
    "Y_binary = np.array(list(zip([1 if i ==0 else 0 for i in Y_train_semEval], Y_train_semEval)))\n",
    "\n",
    "\n",
    "\n",
    "for rd in range(5):\n",
    "\n",
    "    NameTag = 'LSTM_Round{}'.format(rd)\n",
    "\n",
    "\n",
    "    ### train the LogisticRegression classifer and evaluate\n",
    "    X_train = get_feature_arrays(df_train_semEval3)\n",
    "    model = get_model()\n",
    "    batch_size = 64\n",
    "    model.fit(X_train, Y_binary, batch_size=batch_size, epochs=get_n_epochs())\n",
    "\n",
    "    ### evaluate this classifer\n",
    "    X_test = get_feature_arrays(df_test_semEval3)\n",
    "    probs_test = model.predict(X_test)\n",
    "    preds_test = np.array([1 if r[1] > r[0] else 0 for r in probs_test])\n",
    "\n",
    "    df_res_dev = df_res_dev.append({'tag': NameTag,\n",
    "                                    # TODO: add one column : all samples; predicted postive , real postive, overlap postive\n",
    "                                    'Nb:allsamp,predpos,realpos,overlap': [len(preds_test), Counter(preds_test)[1], Counter(Y_test_semEval)[1], \n",
    "                                    len([i for inx, i in enumerate(preds_test) if i == Y_test_semEval[inx]])],\n",
    "                                    # metrics values\n",
    "                                    'accuracy': metrics.accuracy_score(Y_test_semEval, preds_test), \n",
    "                                    'precision': metrics.precision_score(Y_test_semEval, preds_test),\n",
    "                                    'recall': metrics.recall_score(Y_test_semEval, preds_test),\n",
    "                                    'F1': metrics.f1_score(Y_test_semEval, preds_test),\n",
    "                                   }, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    print(\"finished Round {}\".format(rd))\n",
    "\n",
    "# save results to disk\n",
    "df_res_dev.to_pickle(path_here+ \"/res/df_res_dev.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f293ed5-1d75-4df3-a1e2-1331beedb83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a19b5-34b4-405e-8cf9-e5708adf1243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
